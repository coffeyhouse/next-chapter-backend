# core/utils/book_sync_helper.py
from typing import List
from sqlalchemy.orm import Session
from core.sa.repositories.book import BookRepository
from core.sa.models import BookScraped
from core.resolvers.book_creator import BookCreator

def process_book_ids(session: Session, goodreads_ids: List[str], source: str, scrape: bool = False):
    """
    Processes a list of Goodreads IDs:
      - If a book exists in the Book table (by goodreads_id), it is skipped.
      - If a scrape record exists in BookScraped with a work_id that maps to a book in Book, it is skipped.
      - Otherwise, scrape the book data and create a new Book record.
    
    Returns a list of newly created Book objects.
    """
    created_books = []
    book_repo = BookRepository(session)
    creator = BookCreator(session, scrape=scrape)
    
    for gr_id in goodreads_ids:
        # Check if the book already exists by goodreads_id.
        if book_repo.get_by_goodreads_id(gr_id):
            continue
        
        # Check the BookScraped table for an existing scrape record.
        scraped = session.query(BookScraped).filter_by(goodreads_id=gr_id).first()
        if scraped and scraped.work_id:
            if book_repo.get_by_work_id(scraped.work_id):
                continue
        
        # Otherwise, scrape and create the book.
        book = creator.create_book_from_goodreads(gr_id, source=source)
        if book:
            created_books.append(book)
    
    return created_books



# core\utils\http.py

import requests
from pathlib import Path
from urllib.parse import urlparse
from core.utils.proxy.proxy_manager import ProxyManager
from core.utils.rate_limit import RateLimiter

class GoodreadsDownloader:
    def __init__(self, scrape=False):
        self.scrape = scrape
        if self.scrape:
            self.proxy_manager = ProxyManager()
            self.rate_limiter = RateLimiter()
        else:
            self.proxy_manager = None
            self.rate_limiter = None
        self.last_successful_proxy = None
        self.last_successful_headers = None
        
    def download_url(self, url):
        # Apply rate limiting before download if scraping
        if self.scrape and self.rate_limiter:
            self.rate_limiter.delay()
            
        parsed_url = urlparse(url)
        path_parts = parsed_url.path.strip('/').split('/')
        
        local_path = self._get_local_path(parsed_url, path_parts)
        
        # Check if file exists locally
        if local_path.exists():
            return True
            
        # If file doesn't exist and scraping is disabled, skip
        if not self.scrape:
            print(f"Skipping {url} - scraping disabled and no local file found")
            return False
            
        print(f"Scraping enabled, attempting to download {url}")
        return self._try_new_proxies(url, local_path)

    def _get_local_path(self, parsed_url, path_parts):
        path = Path('data/cache')
        full_path = '/'.join(path_parts).rstrip('/')
        query = parsed_url.query if parsed_url.query else ''
        return path / (full_path + query + '.html')

    def _try_new_proxies(self, url, local_path):
        """Try new proxies until one works"""
        if not self.scrape:
            return False
            
        for proxy in self.proxy_manager.get_proxies():
            headers = self.proxy_manager.get_headers()
            proxy_dict = {'http': f'http://{proxy.ip}:{proxy.port}'}
            
            try:
                print(f"\nTrying proxy: {proxy_dict['http']}")
                response = requests.get(
                    url,
                    headers=headers,
                    proxies=proxy_dict,
                    timeout=10
                )
                response.raise_for_status()
                
                # Save successful proxy and headers for future use
                self.last_successful_proxy = proxy_dict
                self.last_successful_headers = headers
                
                local_path.write_text(response.text, encoding='utf-8')
                print(f"Successfully downloaded: {url} to {local_path}")
                return True
                
            except requests.RequestException as e:
                print(f"Proxy failed: {str(e)}")
                continue
                
        print("All proxies failed")
        return False


# core\utils\image.py

import os
from pathlib import Path
import requests
import re
import json
from urllib.parse import urlparse
import logging
from typing import Optional, Tuple
from PIL import Image
from io import BytesIO
import click

logger = logging.getLogger(__name__)

class ImageDownloader:
    def __init__(self, base_dir: str = 'data/images'):
        """Initialize the image downloader with a base directory.
        If base_dir is an absolute path, it will be used as is.
        Otherwise, it will be relative to the current working directory."""
        self.base_dir = Path(base_dir) if os.path.isabs(base_dir) else Path(base_dir)
        
    def _create_directory(self, image_type: str) -> Path:
        """Create and return the appropriate directory for the image type"""
        directory = self.base_dir / image_type if image_type else self.base_dir
        directory.mkdir(parents=True, exist_ok=True)
        return directory
        
    def _process_image(self, image_data: bytes, max_height: int = 300) -> bytes:
        """Process the image to ensure it's a WebP and resize if needed.
        
        Args:
            image_data: Raw image bytes
            max_height: Maximum height in pixels (default: 300)
            
        Returns:
            Processed image as bytes
        """
        # Open image from bytes
        img = Image.open(BytesIO(image_data))
        
        # Convert to RGB if necessary (e.g., if PNG with transparency)
        if img.mode in ('RGBA', 'P'):
            img = img.convert('RGB')
        
        # Calculate new dimensions if height exceeds max_height
        if img.height > max_height:
            ratio = max_height / img.height
            new_width = int(img.width * ratio)
            img = img.resize((new_width, max_height), Image.Resampling.LANCZOS)
        
        # Save to bytes as WebP
        output = BytesIO()
        img.save(output, format='WEBP', quality=85, method=6)
        return output.getvalue()
        
    def _get_extension(self, url: str, content_type: str) -> str:
        """Determine the file extension from URL or content type"""
        # Try to get extension from URL first
        url_path = urlparse(url).path
        ext = Path(url_path).suffix.lower()
        
        if ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
            return ext
            
        # Fall back to content type
        content_type_map = {
            'image/jpeg': '.jpg',
            'image/png': '.png',
            'image/gif': '.gif',
            'image/webp': '.webp'
        }
        
        return content_type_map.get(content_type, '.jpg')  # Default to jpg
        
    def _clean_image_url(self, url: str) -> str:
        """Clean up the image URL"""
        # Convert HTTP to HTTPS
        if url.startswith('http://'):
            url = 'https://' + url[7:]
            
        # Remove compression parameters from Goodreads URLs
        if 'goodreads.com' in url:
            url = re.sub(r'_\w+\d+_', '_', url)  # Remove size indicators like _SX318_
            url = re.sub(r'compressed\.', '', url)  # Remove "compressed." prefix
            
        return url
        
    def _validate_image(self, content: bytes, content_type: str) -> bool:
        """Validate that the content is actually an image"""
        # Check minimum size (1KB)
        if len(content) < 1024:
            return False
            
        # Check content type
        if not content_type.startswith('image/'):
            return False
            
        # Check for common image headers
        image_headers = {
            b'\xff\xd8\xff',  # JPEG
            b'\x89PNG\r\n',   # PNG
            b'GIF87a',        # GIF
            b'GIF89a',        # GIF
            b'RIFF'           # WEBP
        }
        
        return any(content.startswith(header) for header in image_headers)
        
    def download_image(
        self, 
        url: str, 
        image_type: str, 
        identifier: str,
        force_update: bool = False
    ) -> Tuple[bool, Optional[str]]:
        """
        Download an image and save it to the appropriate directory
        
        Args:
            url: The URL of the image to download
            image_type: Type of image (e.g., 'book', 'author') or empty string for base directory
            identifier: Unique identifier for the image (e.g., work_id or author_id)
            force_update: If True, download even if file exists
            
        Returns:
            Tuple of (success: bool, local_path: Optional[str])
        """
        if not url:
            return False, None
            
        try:
            url = self._clean_image_url(url)
            save_dir = self._create_directory(image_type)
            
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            content_type = response.headers.get('content-type', 'image/jpeg')
            if not self._validate_image(response.content, content_type):
                return False, None
            
            # Process image
            processed_image = self._process_image(response.content)
            
            # Always use .jpg extension since we're converting to JPEG
            file_path = save_dir / f"{identifier}.jpg"
            
            if file_path.exists() and not force_update:
                return True, str(file_path)
            
            with open(file_path, 'wb') as f:
                f.write(processed_image)
                
            return True, str(file_path)
            
        except requests.RequestException:
            return False, None
        except Exception as e:
            logger.error(f"Error processing image: {str(e)}")
            return False, None

def download_book_cover(work_id: str, cover_url: str) -> Optional[str]:
    """Download a book cover image using work_id as filename"""
    # Use absolute path to frontend's public covers directory
    covers_dir = "C:/Code/calibre_companion/frontend/public/covers"
    
    try:
        # Create directory if it doesn't exist
        Path(covers_dir).mkdir(parents=True, exist_ok=True)
        
        # Download image
        response = requests.get(cover_url)
        if not response.ok:
            click.echo(f"Failed to download image from {cover_url}: {response.status_code}")
            return None
            
        # Process image
        img = Image.open(BytesIO(response.content))
        
        # Convert to RGB if necessary
        if img.mode in ('RGBA', 'P'):
            img = img.convert('RGB')
        
        # Resize if height exceeds max_height
        max_height = 300
        if img.height > max_height:
            ratio = max_height / img.height
            new_width = int(img.width * ratio)
            img = img.resize((new_width, max_height), Image.Resampling.LANCZOS)
            
        # Save as WebP
        image_path = Path(covers_dir) / f"{work_id}.webp"
        img.save(image_path, format='WEBP', quality=85, method=6)
            
        # Return frontend-relative path
        return f"/covers/{work_id}.webp"
        
    except Exception as e:
        click.echo(f"Error downloading cover: {e}")
        return None

def download_author_photo(author_id: str, photo_url: str) -> Optional[str]:
    """Download an author photo"""
    downloader = ImageDownloader()
    success, path = downloader.download_image(
        url=photo_url,
        image_type='author',
        identifier=author_id
    )
    return path if success else None

# Integration with book.py
def extract_book_cover_url(soup):
    """Extract the book cover image URL from a book page"""
    # Try class ResponsiveImage first
    img = soup.find('img', {'class': 'ResponsiveImage'})
    if img and 'src' in img.attrs:
        return img['src']
        
    # Try schema.org metadata as fallback
    schema_script = soup.find('script', {'type': 'application/ld+json'})
    if schema_script:
        try:
            data = json.loads(schema_script.string)
            if 'image' in data:
                return data['image']
        except json.JSONDecodeError:
            pass
            
    return None

# Example usage:
# Use the main download_book_cover function above
# downloader = ImageDownloader()
# success, path = downloader.download_image(
#     url=cover_url,
#     image_type='book',
#     identifier=book_id
# )

def extract_author_photo_url(soup):
    """Extract the author photo URL from an author page"""
    # Look for author photo in img tag
    img = soup.find('img', {'alt': lambda x: x and 'author' in x.lower()})
    if img and 'src' in img.attrs:
        return img['src']
    
    return None

def download_author_photo(author_id: str, photo_url: str) -> Optional[str]:
    """Download an author photo"""
    downloader = ImageDownloader()
    success, path = downloader.download_image(
        url=photo_url,
        image_type='author',
        identifier=author_id
    )
    return path if success else None


# core\utils\progress_utils.py

import click
from typing import List, Any, Callable, Optional

def create_progress_bar(
    items: List[Any],
    label: str = 'Processing',
    item_name_func: Optional[Callable[[Any], str]] = None,
    show_item_progress: bool = True
) -> click.progressbar:
    """Create a standardized progress bar that shows overall progress and, optionally, the current item."""
    return click.progressbar(
        items,
        label=click.style(label, fg='blue'),
        item_show_func=lambda x: click.style(item_name_func(x), fg='cyan') if (item_name_func and show_item_progress) else None,
        show_eta=True,
        show_percent=True,
        width=50
    )

def nested_progress_bar(total: int, label: str = 'Subtasks') -> click.progressbar:
    """Create a progress bar for tracking subtasks within an item."""
    return click.progressbar(
        range(total),
        label=click.style(label, fg='magenta'),
        show_eta=True,
        show_percent=True,
        width=30
    )



# core/utils/rate_limit.py

import time
import random
from datetime import datetime, timedelta
from typing import Optional
import click

class RateLimiter:
    def __init__(self, 
                 min_delay: float = 1.0,
                 max_delay: float = 1.5,
                 burst_size: int = 50,
                 min_burst_delay: float = 5.0,
                 max_burst_delay: float = 10.0):
        """
        Initialize rate limiter with configurable delays.
        
        Args:
            min_delay: Minimum delay between requests in seconds
            max_delay: Maximum delay between requests in seconds
            burst_size: Number of requests before triggering burst delay
            burst_delay: Delay after burst_size requests in seconds
        """
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.burst_size = burst_size
        self.min_burst_delay = min_burst_delay
        self.max_burst_delay = max_burst_delay
        self.request_count = 0
        self.last_request_time: Optional[datetime] = None

    def delay(self) -> None:
        """Apply appropriate delay before next request"""
        current_time = datetime.now()
        
        # If this is not the first request
        if self.last_request_time:
            # Calculate time since last request
            time_since_last = (current_time - self.last_request_time).total_seconds()
            
            # Determine delay
            if self.request_count >= self.burst_size:
                # Reset counter and apply burst delay
                burst_delay = random.uniform(self.min_burst_delay, self.max_burst_delay)
                click.echo(f"\nTaking a longer break for {burst_delay:.1f} seconds...")
                time.sleep(burst_delay)
                self.request_count = 0
            else:
                # Calculate random delay
                delay = random.uniform(self.min_delay, self.max_delay)
                
                # If we need to wait more
                if time_since_last < delay:
                    wait_time = delay - time_since_last
                    # click.echo(f"\nWaiting {wait_time:.1f} seconds...")
                    time.sleep(wait_time)
        
        self.request_count += 1
        self.last_request_time = datetime.now()

