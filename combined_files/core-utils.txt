# core\utils\http.py

import requests
from pathlib import Path
from urllib.parse import urlparse
from core.utils.proxy.proxy_manager import ProxyManager
from core.utils.rate_limit import RateLimiter

class GoodreadsDownloader:
    def __init__(self, scrape=False):
        self.scrape = scrape
        if self.scrape:
            self.proxy_manager = ProxyManager()
            self.rate_limiter = RateLimiter()
        else:
            self.proxy_manager = None
            self.rate_limiter = None
        self.last_successful_proxy = None
        self.last_successful_headers = None
        
    def download_url(self, url):
        # Apply rate limiting before download if scraping
        if self.scrape and self.rate_limiter:
            self.rate_limiter.delay()
            
        parsed_url = urlparse(url)
        path_parts = parsed_url.path.strip('/').split('/')
        
        local_path = self._get_local_path(parsed_url, path_parts)
        
        # Check if file exists locally
        if local_path.exists():
            return True
            
        # If file doesn't exist and scraping is disabled, skip
        if not self.scrape:
            print(f"Skipping {url} - scraping disabled and no local file found")
            return False
            
        print(f"Scraping enabled, attempting to download {url}")
        return self._try_new_proxies(url, local_path)

    def _get_local_path(self, parsed_url, path_parts):
        path = Path('data/cache')
        full_path = '/'.join(path_parts).rstrip('/')
        query = parsed_url.query if parsed_url.query else ''
        return path / (full_path + query + '.html')

    def _try_new_proxies(self, url, local_path):
        """Try new proxies until one works"""
        if not self.scrape:
            return False
            
        for proxy in self.proxy_manager.get_proxies():
            headers = self.proxy_manager.get_headers()
            proxy_dict = {'http': f'http://{proxy.ip}:{proxy.port}'}
            
            try:
                print(f"\nTrying proxy: {proxy_dict['http']}")
                response = requests.get(
                    url,
                    headers=headers,
                    proxies=proxy_dict,
                    timeout=10
                )
                response.raise_for_status()
                
                # Save successful proxy and headers for future use
                self.last_successful_proxy = proxy_dict
                self.last_successful_headers = headers
                
                local_path.write_text(response.text, encoding='utf-8')
                print(f"Successfully downloaded: {url} to {local_path}")
                return True
                
            except requests.RequestException as e:
                print(f"Proxy failed: {str(e)}")
                continue
                
        print("All proxies failed")
        return False


# core\utils\image.py

import os
from pathlib import Path
import requests
import re
import json
from urllib.parse import urlparse
import logging
from typing import Optional, Tuple

logger = logging.getLogger(__name__)

class ImageDownloader:
    def __init__(self, base_dir: str = 'data/images'):
        """Initialize the image downloader with a base directory"""
        self.base_dir = Path(base_dir)
        
    def _create_directory(self, image_type: str) -> Path:
        """Create and return the appropriate directory for the image type"""
        directory = self.base_dir / image_type
        directory.mkdir(parents=True, exist_ok=True)
        return directory
        
    def _get_extension(self, url: str, content_type: str) -> str:
        """Determine the file extension from URL or content type"""
        # Try to get extension from URL first
        url_path = urlparse(url).path
        ext = Path(url_path).suffix.lower()
        
        if ext in ['.jpg', '.jpeg', '.png', '.gif', '.webp']:
            return ext
            
        # Fall back to content type
        content_type_map = {
            'image/jpeg': '.jpg',
            'image/png': '.png',
            'image/gif': '.gif',
            'image/webp': '.webp'
        }
        
        return content_type_map.get(content_type, '.jpg')  # Default to jpg
        
    def _clean_image_url(self, url: str) -> str:
        """Clean up the image URL"""
        # Convert HTTP to HTTPS
        if url.startswith('http://'):
            url = 'https://' + url[7:]
            
        # Remove compression parameters from Goodreads URLs
        if 'goodreads.com' in url:
            url = re.sub(r'_\w+\d+_', '_', url)  # Remove size indicators like _SX318_
            url = re.sub(r'compressed\.', '', url)  # Remove "compressed." prefix
            
        return url
        
    def _validate_image(self, content: bytes, content_type: str) -> bool:
        """Validate that the content is actually an image"""
        # Check minimum size (1KB)
        if len(content) < 1024:
            return False
            
        # Check content type
        if not content_type.startswith('image/'):
            return False
            
        # Check for common image headers
        image_headers = {
            b'\xff\xd8\xff',  # JPEG
            b'\x89PNG\r\n',   # PNG
            b'GIF87a',        # GIF
            b'GIF89a',        # GIF
            b'RIFF'           # WEBP
        }
        
        return any(content.startswith(header) for header in image_headers)
        
    def download_image(
        self, 
        url: str, 
        image_type: str, 
        identifier: str,
        force_update: bool = False
    ) -> Tuple[bool, Optional[str]]:
        """
        Download an image and save it to the appropriate directory
        
        Args:
            url: The URL of the image to download
            image_type: Type of image (e.g., 'book', 'author')
            identifier: Unique identifier for the image (e.g., book_id or author_id)
            force_update: If True, download even if file exists
            
        Returns:
            Tuple of (success: bool, local_path: Optional[str])
        """
        if not url:
            return False, None
            
        try:
            url = self._clean_image_url(url)
            save_dir = self._create_directory(image_type)
            
            response = requests.get(url, timeout=10)
            response.raise_for_status()
            
            content_type = response.headers.get('content-type', 'image/jpeg')
            if not self._validate_image(response.content, content_type):
                return False, None
                
            extension = self._get_extension(url, content_type)
            file_path = save_dir / f"{identifier}{extension}"
            
            if file_path.exists() and not force_update:
                return True, str(file_path)
            
            with open(file_path, 'wb') as f:
                f.write(response.content)
                
            return True, str(file_path)
            
        except requests.RequestException:
            return False, None
        except Exception:
            return False, None

def download_book_cover(book_id: str, cover_url: str) -> Optional[str]:
    """Download a book cover image"""
    downloader = ImageDownloader()
    success, path = downloader.download_image(
        url=cover_url,
        image_type='book',
        identifier=book_id
    )
    return path if success else None

def download_author_photo(author_id: str, photo_url: str) -> Optional[str]:
    """Download an author photo"""
    downloader = ImageDownloader()
    success, path = downloader.download_image(
        url=photo_url,
        image_type='author',
        identifier=author_id
    )
    return path if success else None

# Integration with book.py
def extract_book_cover_url(soup):
    """Extract the book cover image URL from a book page"""
    # Try class ResponsiveImage first
    img = soup.find('img', {'class': 'ResponsiveImage'})
    if img and 'src' in img.attrs:
        return img['src']
        
    # Try schema.org metadata as fallback
    schema_script = soup.find('script', {'type': 'application/ld+json'})
    if schema_script:
        try:
            data = json.loads(schema_script.string)
            if 'image' in data:
                return data['image']
        except json.JSONDecodeError:
            pass
            
    return None

# Example usage:
def download_book_cover(book_id: str, cover_url: str) -> Optional[str]:
    """Download a book cover image"""
    downloader = ImageDownloader()
    success, path = downloader.download_image(
        url=cover_url,
        image_type='book',
        identifier=book_id
    )
    return path if success else None

def extract_author_photo_url(soup):
    """Extract the author photo URL from an author page"""
    # Look for author photo in img tag
    img = soup.find('img', {'alt': lambda x: x and 'author' in x.lower()})
    if img and 'src' in img.attrs:
        return img['src']
    
    return None

def download_author_photo(author_id: str, photo_url: str) -> Optional[str]:
    """Download an author photo"""
    downloader = ImageDownloader()
    success, path = downloader.download_image(
        url=photo_url,
        image_type='author',
        identifier=author_id
    )
    return path if success else None


# core/utils/rate_limit.py

import time
import random
from datetime import datetime, timedelta
from typing import Optional
import click

class RateLimiter:
    def __init__(self, 
                 min_delay: float = 0.2,
                 max_delay: float = 0.3,
                 burst_size: int = 150,
                 min_burst_delay: float = 1.0,
                 max_burst_delay: float = 3.0):
        """
        Initialize rate limiter with configurable delays.
        
        Args:
            min_delay: Minimum delay between requests in seconds
            max_delay: Maximum delay between requests in seconds
            burst_size: Number of requests before triggering burst delay
            burst_delay: Delay after burst_size requests in seconds
        """
        self.min_delay = min_delay
        self.max_delay = max_delay
        self.burst_size = burst_size
        self.min_burst_delay = min_burst_delay
        self.max_burst_delay = max_burst_delay
        self.request_count = 0
        self.last_request_time: Optional[datetime] = None

    def delay(self) -> None:
        """Apply appropriate delay before next request"""
        current_time = datetime.now()
        
        # If this is not the first request
        if self.last_request_time:
            # Calculate time since last request
            time_since_last = (current_time - self.last_request_time).total_seconds()
            
            # Determine delay
            if self.request_count >= self.burst_size:
                # Reset counter and apply burst delay
                burst_delay = random.uniform(self.min_burst_delay, self.max_burst_delay)
                click.echo(f"\nTaking a longer break for {burst_delay:.1f} seconds...")
                time.sleep(burst_delay)
                self.request_count = 0
            else:
                # Calculate random delay
                delay = random.uniform(self.min_delay, self.max_delay)
                
                # If we need to wait more
                if time_since_last < delay:
                    wait_time = delay - time_since_last
                    # click.echo(f"\nWaiting {wait_time:.1f} seconds...")
                    time.sleep(wait_time)
        
        self.request_count += 1
        self.last_request_time = datetime.now()

