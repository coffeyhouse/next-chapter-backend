# core/scrapers/author_books_scraper.py
from bs4 import BeautifulSoup
from pathlib import Path
import re
from urllib.parse import urlencode
from ..utils.http import GoodreadsDownloader

class AuthorBooksScraper:
   """Scrapes list of books by an author"""
   
   def __init__(self, scrape: bool = False):
       self.downloader = GoodreadsDownloader(scrape)
   
   def scrape_author_books(self, author_id: str) -> dict:
       """
       Get list of all books by author
       Expected output:
       {
           'author_id': str,
           'author_name': str,
           'books': [
               {
                   'goodreads_id': str,
                   'title': str,
                   'published_date': str
               }
           ]
       }
       """
       print(f"Scraping books for author: {author_id}")
       books = []
       author_name = None
       current_page = 1
       
       while True:
           # Get page content
           url = self._get_page_url(author_id, current_page)
           if not self.downloader.download_url(url):
               if books:  # Return what we have if not first page
                   break
               return None
           
           # Read the downloaded page
           html = self._read_html(author_id, current_page)
           if not html:
               if books:
                   break
               return None
               
           try:
               # Parse page content
               soup = BeautifulSoup(html, 'html.parser')
               
               # Get author name on first page
               if not author_name:
                   author_name = self._extract_author_name(soup)
               
               # Get books from this page
               page_books = self._extract_books(soup)
               books.extend(page_books)
               
               # Check pagination
               pagination = self._extract_pagination(soup)
               print(f"Processing page {pagination['current_page']} of {pagination['total_pages']}")
               
               if current_page >= pagination['total_pages']:
                   break
                   
               current_page += 1
               
           except Exception as e:
               print(f"Error processing page {current_page}: {e}")
               if books:
                   break
               return None
       
       # Filter to only include books with dates
       dated_books = [b for b in books if b.get('published_date')]
       
       return {
           'author_id': author_id,
           'author_name': author_name,
           'books': dated_books
       }
   
   def _get_page_url(self, author_id: str, page: int) -> str:
       """Get URL for author's books page"""
       base = f"https://www.goodreads.com/author/list/{author_id}"
       params = {
           'page': page,
           'per_page': 100,
           'utf8': 'âœ“',
           'sort': 'original_publication_year'
       }
       return f"{base}?{urlencode(params)}"
   
   def _read_html(self, author_id: str, page: int) -> str:
       """Read downloaded HTML file"""
       query = f"page={page}&per_page=100&utf8=%E2%9C%93&sort=original_publication_year"
       path = Path('data/cache/author/list') / f"{author_id}{query}.html"
       try:
           with open(path, 'r', encoding='utf-8') as f:
               return f.read()
       except Exception as e:
           print(f"Error reading HTML file: {e}")
           return None
   
   def _extract_author_name(self, soup) -> str:
       """Extract author name"""
       name_link = soup.find('a', class_='authorName')
       if name_link:
           return name_link.text.strip()
       return None
   
   def _extract_books(self, soup) -> list:
       """Extract books from page"""
       books = []
       book_rows = soup.find_all('tr', itemtype='http://schema.org/Book')
       
       for row in book_rows:
           book_link = row.find('a', class_='bookTitle')
           if not book_link:
               continue
               
           # Get title and ID
           book = {
               'title': book_link.find('span', itemprop='name').text.strip(),
               'goodreads_id': None,
               'published_date': None
           }
           
           # Get book ID from URL
           url_match = re.search(r'/show/(\d+)', book_link['href'])
           if url_match:
               book['goodreads_id'] = url_match.group(1)
               
           # Get publication date
           grey_text = row.find('span', class_='greyText smallText uitext')
           if grey_text:
               text = grey_text.get_text(strip=True)
               for prefix in ['expected publication', 'published']:
                   if prefix in text.lower():
                       year_match = re.search(rf'{prefix}\s*(\d{{4}})', text, re.I)
                       if year_match:
                           book['published_date'] = year_match.group(1)
                           break
           
           if book['goodreads_id'] and book['title']:
               books.append(book)
               
       return books
   
   def _extract_pagination(self, soup) -> dict:
       """Extract pagination information"""
       pagination = {'current_page': 1, 'total_pages': 1}
       
       # Find pagination div
       div = soup.find('div', style='float: right')
       if div:
           # Get current page
           current = div.find('em', class_='current')
           if current:
               try:
                   pagination['current_page'] = int(current.text.strip())
               except ValueError:
                   pass
           
           # Get max page number
           max_page = 1
           for link in div.find_all('a'):
               try:
                   page_num = int(link.text.strip())
                   max_page = max(max_page, page_num)
               except ValueError:
                   continue
                   
           pagination['total_pages'] = max_page
           
       return pagination


# core/scrapers/author_scraper.py
from bs4 import BeautifulSoup
from pathlib import Path
from ..utils.http import GoodreadsDownloader
from ..utils.image import download_author_photo

class AuthorScraper:
    """Scrapes author details from Goodreads"""
    
    def __init__(self, scrape: bool = False):
        self.downloader = GoodreadsDownloader(scrape)
    
    def scrape_author(self, author_id: str) -> dict:
        """
        Get author data from author page
        Expected output matches authors table schema:
        {
            'goodreads_id': str,
            'name': str,
            'bio': str,
            'image_url': str
        }
        """
        print(f"Scraping author: {author_id}")
        
        # Get author page
        url = self._get_author_url(author_id)
        if not self.downloader.download_url(url):
            print(f"Failed to download author page for ID: {author_id}")
            return None
            
        # Read HTML
        html = self._read_html(author_id)
        if not html:
            return None
            
        try:
            soup = BeautifulSoup(html, 'html.parser')
            author_data = {
                'goodreads_id': author_id,
                'name': self._extract_name(soup),
                'bio': self._extract_bio(soup),
                'image_url': None
            }
            
            # Handle author photo
            photo_url = self._extract_photo_url(soup)
            if photo_url:
                local_path = download_author_photo(author_id, photo_url)
                if local_path:
                    author_data['image_url'] = local_path
            
            return author_data
            
        except Exception as e:
            print(f"Error parsing author data: {e}")
            return None
    
    def _get_author_url(self, author_id: str) -> str:
        """Get Goodreads URL for author"""
        return f"https://www.goodreads.com/author/show/{author_id}"
    
    def _read_html(self, author_id: str) -> str:
        """Read downloaded HTML file"""
        path = Path('data/cache/author/show') / f"{author_id}.html"
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"Error reading HTML file: {e}")
            return None
    
    def _extract_name(self, soup) -> str:
        """Extract author name"""
        name_element = soup.find('h1', class_='authorName')
        if name_element:
            name_span = name_element.find('span', itemprop='name')
            if name_span:
                return ' '.join(name_span.text.split())
        return None
    
    def _extract_bio(self, soup) -> str:
        """Extract author biography"""
        bio_div = soup.find('div', class_='aboutAuthorInfo')
        if bio_div:
            bio_span = bio_div.find('span', id=lambda x: x and x.startswith('freeTextContainer'))
            if bio_span:
                return bio_span.text.strip()
        return None
    
    def _extract_photo_url(self, soup) -> str:
        """Extract author photo URL with fallbacks"""
        # Try authorPhotoImg class first
        img = soup.find('img', class_='authorPhotoImg')
        if img and 'src' in img.attrs:
            return img['src']
        
        # Try alt text containing author
        img = soup.find('img', alt=lambda x: x and 'author' in x.lower())
        if img and 'src' in img.attrs:
            return img['src']
        
        # Try leftContainer
        container = soup.find('div', class_='leftContainer authorLeftContainer')
        if container:
            img = container.find('img')
            if img and 'src' in img.attrs:
                return img['src']
        
        return None


# core\scrapers\base_scraper.py

from bs4 import BeautifulSoup
from pathlib import Path
import logging
from typing import Optional, Union
import click
from abc import ABC, abstractmethod
from ..utils.http import GoodreadsDownloader
import time
import json
from datetime import datetime, timedelta
from urllib.parse import urlencode

class BaseScraper(ABC):
    """Base class for all scrapers providing common functionality."""
    
    def __init__(self, scrape: bool = False, cache_dir: Union[str, Path] = 'data/cache', cache_max_age: int = 3600):
        """
        Initialize the base scraper.
        
        Args:
            scrape: Whether to allow live scraping or use only cached data
            cache_dir: Base directory for caching scraped data
            cache_max_age: Maximum age in seconds for cache files to be considered fresh
        """
        self.downloader = GoodreadsDownloader(scrape)
        self.cache_dir = Path(cache_dir)
        self.cache_max_age = cache_max_age  # seconds
        self.cache_dir.mkdir(parents=True, exist_ok=True)
        self._setup_logging()

    def _setup_logging(self):
        """Set up logging for the scraper."""
        self.logger = logging.getLogger(self.__class__.__name__)
        if not self.logger.handlers:
            handler = logging.StreamHandler()
            formatter = logging.Formatter(
                '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
            )
            handler.setFormatter(formatter)
            self.logger.addHandler(handler)
            self.logger.setLevel(logging.INFO)

    def build_url(self, base: str, params: dict) -> str:
        """
        Construct a URL with query parameters.
        
        Args:
            base: The base URL.
            params: Dictionary of query parameters.
            
        Returns:
            The constructed URL as a string.
        """
        return f"{base}?{urlencode(params)}"

    def get_cache_path(self, identifier: str, subdir: str = '', suffix: str = '.html') -> Path:
        """
        Get the cache file path for an identifier.
        
        Args:
            identifier: Unique identifier for the item.
            subdir: Optional subdirectory within cache.
            suffix: File suffix to use.
            
        Returns:
            Path: The constructed cache path.
        """
        path = self.cache_dir
        if subdir:
            path = path / subdir
        return path / f"{identifier}{suffix}"

    def read_cache(self, cache_path: Union[str, Path]) -> Optional[str]:
        """
        Read HTML content from cache.
        
        Args:
            cache_path: Path to the cached file.
            
        Returns:
            The cached HTML content as a string, or None if not found/error.
        """
        try:
            with open(cache_path, 'r', encoding='utf-8') as f:
                return f.read()
        except FileNotFoundError:
            self.logger.warning(f"Cache file not found: {cache_path}")
            return None
        except Exception as e:
            self.logger.error(f"Error reading cache {cache_path}: {e}")
            return None

    def write_cache(self, cache_path: Union[str, Path], content: str) -> bool:
        """
        Write content to cache.
        
        Args:
            cache_path: Path where to write the cache file.
            content: The content to cache.
            
        Returns:
            True if write was successful, False otherwise.
        """
        try:
            cache_path = Path(cache_path)
            cache_path.parent.mkdir(parents=True, exist_ok=True)
            with open(cache_path, 'w', encoding='utf-8') as f:
                f.write(content)
            return True
        except Exception as e:
            self.logger.error(f"Error writing cache {cache_path}: {e}")
            return False

    def is_cache_fresh(self, cache_path: Union[str, Path]) -> bool:
        """
        Check if a cache file is fresh based on its modification time.
        
        Args:
            cache_path: Path to the cached file.
            
        Returns:
            True if the cache file exists and is not older than self.cache_max_age.
        """
        cache_path = Path(cache_path)
        if not cache_path.exists():
            return False
        file_mtime = datetime.fromtimestamp(cache_path.stat().st_mtime)
        if datetime.now() - file_mtime < timedelta(seconds=self.cache_max_age):
            return True
        return False

    def download_url(self, url: str, identifier: str, retries: int = 3) -> bool:
        """
        Download a URL and cache the response using an exponential backoff strategy.
        
        Args:
            url: The URL to download.
            identifier: Unique identifier for caching.
            retries: Number of download attempts before giving up.
            
        Returns:
            True if download was successful, False otherwise.
        """
        if click.get_current_context().find_root().params.get('verbose', False):
            click.echo(click.style(f"Downloading: {url}", fg='cyan'))
            
        attempt = 0
        delay = 1  # initial delay in seconds
        while attempt < retries:
            try:
                success = self.downloader.download_url(url)
                if success:
                    return True
                attempt += 1
                self.logger.warning(f"Download attempt {attempt} failed for {url}. Retrying in {delay} seconds.")
                time.sleep(delay)
                delay *= 2  # exponential backoff
            except Exception as e:
                self.logger.error(f"Error downloading {url}: {e}")
                attempt += 1
                time.sleep(delay)
                delay *= 2
        
        self.logger.error(f"Failed to download {url} after {retries} attempts")
        return False

    def parse_html(self, html: str) -> Optional[BeautifulSoup]:
        """
        Parse HTML content into a BeautifulSoup object.
        
        Args:
            html: The HTML content to parse.
            
        Returns:
            A BeautifulSoup object if parsing is successful, otherwise None.
        """
        if not html:
            return None
        try:
            return BeautifulSoup(html, 'html.parser')
        except Exception as e:
            self.logger.error(f"Error parsing HTML: {e}")
            return None

    def clean_html(self, html: str) -> str:
        """
        Clean HTML content before parsing.
        Can be overridden by derived classes for specific cleaning needs.
        
        Args:
            html: The HTML content to clean.
            
        Returns:
            The cleaned HTML as a string.
        """
        # Add any common cleaning steps here (e.g., remove unwanted tags)
        return html.strip()

    def extract_json(self, element: Optional[BeautifulSoup], default: Optional[dict] = None) -> dict:
        """
        Safely extract and parse JSON data from a BeautifulSoup element.
        
        Args:
            element: The BeautifulSoup element containing JSON.
            default: Default value to return if extraction fails.
            
        Returns:
            Parsed JSON as a dictionary.
        """
        if element is None:
            return default or {}
        try:
            return json.loads(element.string)
        except Exception as e:
            self.logger.error(f"Error extracting JSON: {e}")
            return default or {}

    @abstractmethod
    def get_url(self, identifier: str) -> str:
        """
        Get the URL for an identifier.
        Must be implemented by derived classes.
        
        Args:
            identifier: The identifier to get the URL for.
            
        Returns:
            The constructed URL as a string.
        """
        pass

    @abstractmethod
    def extract_data(self, soup: BeautifulSoup, identifier: str) -> dict:
        """
        Extract data from parsed HTML.
        Must be implemented by derived classes.
        
        Args:
            soup: The parsed HTML.
            identifier: The identifier being scraped.
            
        Returns:
            A dictionary containing the extracted data.
        """
        pass

    def scrape(self, identifier: str) -> Optional[dict]:
        """
        Main scraping method that coordinates the scraping process.
        
        Args:
            identifier: The identifier of the item to scrape.
            
        Returns:
            A dictionary with the scraped data, or None if scraping failed.
        """
        url = self.get_url(identifier)
        cache_path = self.get_cache_path(identifier)
        
        # Use cache if it exists and is fresh
        if self.is_cache_fresh(cache_path):
            self.logger.info(f"Using fresh cache for {identifier}")
        else:
            # If cache is stale or missing, download and write to cache
            if not self.download_url(url, identifier):
                self.logger.error(f"Failed to download {url}")
                return None
        
        html = self.read_cache(cache_path)
        if not html:
            self.logger.error(f"Failed to read cache for {identifier}")
            return None
        
        html = self.clean_html(html)
        soup = self.parse_html(html)
        if not soup:
            self.logger.error(f"Failed to parse HTML for {identifier}")
            return None
        
        try:
            return self.extract_data(soup, identifier)
        except Exception as e:
            self.logger.error(f"Error extracting data for {identifier}: {e}")
            return None

    def scrape_paginated(self, identifier: str, max_pages: int = None) -> Optional[dict]:
        """
        Scrape data from paginated content.
        
        Args:
            identifier: The identifier of the item to scrape
            max_pages: Maximum number of pages to scrape (None for all pages)
            
        Returns:
            Dictionary containing the combined data from all pages, or None if scraping failed
        """
        all_items = []
        current_page = 1
        metadata = {}  # Store any additional data from first page
        
        while True:
            # Check page limit
            if max_pages and current_page > max_pages:
                break
                
            # Get URL for current page
            url = self.get_page_url(identifier, current_page)
            cache_path = self.get_cache_path(
                f"{identifier}_page_{current_page}", 
                suffix=f"_{urlencode(self.get_pagination_params(current_page))}.html"
            )
            
            # Use cache if fresh, otherwise download
            if not self.is_cache_fresh(cache_path):
                if not self.download_url(url, identifier):
                    if all_items:  # Return what we have if not first page
                        break
                    self.logger.error(f"Failed to download page {current_page}")
                    return None
            
            # Read and parse page
            html = self.read_cache(cache_path)
            if not html:
                if all_items:
                    break
                return None
                
            html = self.clean_html(html)
            soup = self.parse_html(html)
            if not soup:
                if all_items:
                    break
                return None
            
            try:
                # Extract items from current page
                page_data = self.extract_page_data(soup, identifier)
                if not page_data:
                    break
                    
                # Store metadata from first page
                if current_page == 1:
                    metadata = self.extract_metadata(soup, identifier)
                
                # Add items from this page
                all_items.extend(page_data)
                
                # Check pagination
                pagination = self.extract_pagination(soup)
                if not pagination or current_page >= pagination.get('total_pages', current_page):
                    break
                    
                current_page += 1
                
            except Exception as e:
                self.logger.error(f"Error processing page {current_page}: {e}")
                if all_items:
                    break
                return None
        
        # Combine items with metadata
        return {
            'items': all_items,
            **metadata
        }

    @abstractmethod
    def get_page_url(self, identifier: str, page: int) -> str:
        """
        Get URL for a specific page.
        Must be implemented by derived classes.
        
        Args:
            identifier: The identifier being scraped
            page: The page number
            
        Returns:
            The URL for the specified page
        """
        pass

    def get_pagination_params(self, page: int) -> dict:
        """
        Get pagination parameters for URL.
        Can be overridden by derived classes.
        
        Args:
            page: The page number
            
        Returns:
            Dictionary of pagination parameters
        """
        return {
            'page': page
        }

    @abstractmethod
    def extract_page_data(self, soup: BeautifulSoup, identifier: str) -> list:
        """
        Extract items from a single page.
        Must be implemented by derived classes.
        
        Args:
            soup: The parsed HTML
            identifier: The identifier being scraped
            
        Returns:
            List of items from the page
        """
        pass

    def extract_metadata(self, soup: BeautifulSoup, identifier: str) -> dict:
        """
        Extract metadata from first page.
        Can be overridden by derived classes.
        
        Args:
            soup: The parsed HTML
            identifier: The identifier being scraped
            
        Returns:
            Dictionary of metadata
        """
        return {}

    def extract_pagination(self, soup: BeautifulSoup) -> Optional[dict]:
        """
        Extract pagination information.
        Can be overridden by derived classes.
        
        Args:
            soup: The parsed HTML
            
        Returns:
            Dictionary with pagination info (e.g., {'current_page': 1, 'total_pages': 10})
        """
        # Default implementation looks for common pagination patterns
        try:
            pagination = {'current_page': 1, 'total_pages': 1}
            
            # Find pagination container
            paginator = soup.find('div', class_=['pagination', 'paginator'])
            if not paginator:
                paginator = soup.find('div', style='float: right')  # Goodreads style
            
            if paginator:
                # Get current page
                current = paginator.find(['em', 'span'], class_='current')
                if current:
                    try:
                        pagination['current_page'] = int(current.text.strip())
                    except ValueError:
                        pass
                
                # Get max page from links
                max_page = 1
                for link in paginator.find_all('a'):
                    try:
                        page_num = int(link.text.strip())
                        max_page = max(max_page, page_num)
                    except ValueError:
                        continue
                
                pagination['total_pages'] = max_page
            
            return pagination
            
        except Exception as e:
            self.logger.error(f"Error extracting pagination: {e}")
            return None


# core/scrapers/book_scraper.py
from bs4 import BeautifulSoup
from pathlib import Path
import re
import json
from datetime import datetime
import click
from ..utils.http import GoodreadsDownloader
from ..utils.image import download_book_cover

class BookScraper:
    """Scrapes a book page from Goodreads"""
    
    def __init__(self, scrape: bool = False):
        self.downloader = GoodreadsDownloader(scrape)
        
    def scrape(self, book_id: str) -> dict:
        """
        Get book data from Goodreads book page
        Expected output:
        {
            'goodreads_id': str,
            'title': str,
            'work_id': str,
            'published_date': str,
            'published_state': str,
            'language': str,
            'pages': int,
            'isbn': str,
            'goodreads_rating': float,
            'goodreads_votes': int,
            'description': str,
            'image_url': str,
            'source': str,
            'hidden': bool,
            # Relationships (not in books table)
            'authors': [
                {
                    'goodreads_id': str,
                    'name': str,
                    'role': str
                }
            ],
            'series': [
                {
                    'goodreads_id': str,
                    'name': str,
                    'order': float
                }
            ],
            'genres': [
                {
                    'name': str
                }
            ]
        }
        """
        if click.get_current_context().find_root().params.get('verbose', False):
            click.echo(click.style(f"Scraping book: {book_id}", fg='cyan'))
        
        # Get book page content
        url = self._get_book_url(book_id)
        if not self.downloader.download_url(url):
            click.echo(click.style(f"Failed to download book page for ID: {book_id}", fg='red'), err=True)
            return None
            
        # Parse HTML
        html = self._read_html(book_id)
        if not html:
            return None
            
        try:
            soup = BeautifulSoup(html, 'html.parser')
            book_data = {
                'goodreads_id': book_id,
                'title': self._extract_title(soup),
                'work_id': self._extract_work_id(soup),
                'description': self._extract_description(soup),
                'language': None,
                'pages': None,
                'isbn': None,
                'goodreads_rating': None,
                'goodreads_votes': None,
                'published_date': None,
                'published_state': None,
                'image_url': None,
                'source': 'scrape',
                'hidden': False,
                'format': None
            }
            
            # Extract book details
            details = self._extract_book_details(soup)
            book_data.update({
                'language': details.get('language'),
                'pages': details.get('pages'),
                'isbn': details.get('isbn'),
                'goodreads_rating': details.get('rating'),
                'goodreads_votes': details.get('rating_count'),
                'format': details.get('format')
            })
            
            # Get publication date
            pub_info = self._extract_publication_info(soup)
            book_data['published_date'] = pub_info.get('date')
            book_data['published_state'] = pub_info.get('state')
            
            # Get relationships
            book_data['authors'] = self._extract_authors(soup)
            book_data['series'] = self._extract_series(soup)
            book_data['genres'] = self._extract_genres(soup)
            
            # Get cover image
            cover_url = self._extract_cover_url(soup)
            if cover_url:
                local_path = download_book_cover(book_data['work_id'], cover_url)
                if local_path:
                    book_data['image_url'] = local_path
            
            return book_data
            
        except Exception as e:
            click.echo(click.style(f"Error parsing book data: {e}", fg='red'), err=True)
            return None
    
    def _get_book_url(self, book_id: str) -> str:
        """Get Goodreads URL for book"""
        return f"https://www.goodreads.com/book/show/{book_id}"
    
    def _read_html(self, book_id: str) -> str:
        """Read downloaded HTML file"""
        path = Path('data/cache/book/show') / f"{book_id}.html"
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            click.echo(click.style(f"Error reading HTML file: {e}", fg='red'), err=True)
            return None
    
    def _extract_title(self, soup) -> str:
        """Extract book title"""
        title_element = soup.find('h1', attrs={'data-testid': 'bookTitle'})
        if title_element:
            return title_element['aria-label'].replace('Book title:', '').strip()
        return None
    
    def _extract_description(self, soup) -> str:
        """Extract book description"""
        next_data = soup.find('script', id='__NEXT_DATA__')
        if next_data:
            try:
                data = json.loads(next_data.string)
                apollo_state = data['props']['pageProps']['apolloState']
                
                # Find the book ID from the query
                book_id = None
                for key, value in apollo_state.items():
                    if key == 'ROOT_QUERY':
                        for query_key in value:
                            if 'getBookByLegacyId' in query_key:
                                book_id = value[query_key]['__ref']
                                break
                
                # Get description using book ID
                if book_id and book_id in apollo_state:
                    return apollo_state[book_id].get('description')
                    
            except (json.JSONDecodeError, KeyError):
                pass
        return None
    def _extract_book_details(self, soup) -> dict:
        """Extract book details from schema.org data"""
        schema = soup.find('script', {'type': 'application/ld+json'})
        if schema:
            try:
                data = json.loads(schema.string)
                return {
                    'format': data.get('bookFormat'),
                    'language': data.get('inLanguage'),
                    'pages': data.get('numberOfPages'),
                    'isbn': data.get('isbn'),
                    'rating': data.get('aggregateRating', {}).get('ratingValue'),
                    'rating_count': data.get('aggregateRating', {}).get('ratingCount')
                }
            except json.JSONDecodeError:
                pass
        return {}
    
    def _extract_publication_info(self, soup) -> dict:
        pub_element = soup.find('p', attrs={'data-testid': 'publicationInfo'})
        if pub_element:
            text = pub_element.text.strip()
            result = {'date': None, 'state': None}

            # Determine the raw date string based on the prefix
            if text.startswith('Expected publication'):
                result['state'] = 'upcoming'
                raw_date = text.replace('Expected publication', '').strip()
            elif text.startswith('First published'):
                result['state'] = 'published'
                raw_date = text.replace('First published', '').strip()
            elif text.startswith('Published'):
                result['state'] = 'published'
                raw_date = text.replace('Published', '').strip()
            else:
                raw_date = text

            # Convert the raw date to ISO 8601 format if possible.
            try:
                # Adjust the format string if your raw date differs.
                dt = datetime.strptime(raw_date, "%B %d, %Y")
                # This produces something like "2021-08-05T00:00:00.000000"
                result['date'] = dt.isoformat(timespec='microseconds')
            except Exception as e:
                # If parsing fails, fall back to the raw string
                result['date'] = raw_date

            return result
        return {}
    
    def _extract_authors(self, soup) -> list:
        """Extract unique author information"""
        seen_ids = set()  # Track seen author IDs
        authors = []
        author_links = soup.find_all('a', class_='ContributorLink')
        
        for link in author_links:
            name_span = link.find('span', class_='ContributorLink__name')
            if not name_span:
                continue
                
            # Get author ID first
            goodreads_id = None
            if 'href' in link.attrs:
                id_match = re.search(r'/author/show/(\d+)', link['href'])
                if id_match:
                    goodreads_id = id_match.group(1)
            
            # Skip if we don't have a goodreads_id or if we've seen this ID before
            if not goodreads_id or goodreads_id in seen_ids:
                continue
                
            author = {
                'name': name_span.text.strip(),
                'goodreads_id': goodreads_id,
                'role': 'Author'
            }
            
            # Get role if specified
            role_span = link.find('span', class_='ContributorLink__role')
            if role_span:
                role = role_span.text.strip('()').strip()
                # Clean up any remaining parentheses and whitespace
                role = role.replace('(', '').replace(')', '').strip()
                author['role'] = role
            
            authors.append(author)
            seen_ids.add(goodreads_id)
                
        return authors
    
    def _extract_series(self, soup) -> list:
        """Extract all series information (main and additional)"""
        series = []
        
        # First get the main series
        series_element = soup.find('h3', class_='Text__title3', 
                                attrs={'aria-label': lambda x: x and 'Book' in x and 'series' in x})
        
        main_series_id = None
        if series_element:
            series_link = series_element.find('a')
            if series_link:
                # Get series ID
                id_match = re.search(r'/series/(\d+)', series_link['href'])
                if id_match:
                    main_series_id = id_match.group(1)
                    text = series_link.text.strip()
                    
                    # Handle series order if present
                    name = text
                    order = None
                    if '#' in text:
                        name, number = text.split('#')
                        number = number.strip()
                        order = number
                    
                    series.append({
                        'goodreads_id': main_series_id,
                        'name': name.strip(),
                        'order': order
                    })
        
        # Then get additional series
        next_data = soup.find('script', id='__NEXT_DATA__')
        if next_data:
            try:
                data = json.loads(next_data.string)
                book_data = data.get('props', {}).get('pageProps', {}).get('apolloState', {})
                
                # Get all series
                for key, value in book_data.items():
                    if isinstance(value, dict) and value.get('__typename') == 'Series':
                        url = value.get('webUrl', '')
                        series_match = re.search(r'/series/(\d+)', url)
                        series_id = series_match.group(1) if series_match else None
                        
                        # Skip if this is the main series
                        if main_series_id and str(main_series_id) == str(series_id):
                            continue
                        
                        # Add to series list with consistent format
                        series.append({
                            'goodreads_id': series_id,
                            'name': value.get('title', ''),
                            'order': None,  # Additional series typically don't have order
                            'order_str': None
                        })
                        
            except json.JSONDecodeError:
                pass
        
        return series
    
    def _extract_genres(self, soup) -> list:
        """Extract genre information"""
        genres = []
        next_data = soup.find('script', id='__NEXT_DATA__')
        if next_data:
            try:
                data = json.loads(next_data.string)
                book_data = data['props']['pageProps']['apolloState']
                
                for value in book_data.values():
                    if isinstance(value, dict) and 'bookGenres' in value:
                        for genre_data in value['bookGenres']:
                            if isinstance(genre_data, dict) and 'genre' in genre_data:
                                genre = genre_data['genre']
                                if isinstance(genre, dict):
                                    genres.append({
                                        'name': genre.get('name', '')
                                    })
                        break
            except (json.JSONDecodeError, KeyError):
                pass
        return genres
    
    def _extract_cover_url(self, soup) -> str:
        """Extract book cover URL"""
        # Try to get from __NEXT_DATA__ first (new Goodreads structure)
        next_data = soup.find('script', id='__NEXT_DATA__')
        if next_data:
            try:
                data = json.loads(next_data.string)
                apollo_state = data['props']['pageProps']['apolloState']
                
                # Find the book ID from the query
                book_id = None
                for key, value in apollo_state.items():
                    if key == 'ROOT_QUERY':
                        for query_key in value:
                            if 'getBookByLegacyId' in query_key:
                                book_id = value[query_key]['__ref']
                                break
                
                # Get image URL using book ID
                if book_id and book_id in apollo_state:
                    image_url = apollo_state[book_id].get('imageUrl')
                    if image_url:
                        return image_url
            except (json.JSONDecodeError, KeyError):
                pass
                
        # Try ResponsiveImage class
        img = soup.find('img', {'class': 'ResponsiveImage'})
        if img and 'src' in img.attrs:
            return img['src']
            
        # Try schema.org data
        schema = soup.find('script', {'type': 'application/ld+json'})
        if schema:
            try:
                data = json.loads(schema.string)
                image_url = data.get('image')
                if image_url:
                    return image_url
            except json.JSONDecodeError:
                pass
                
        # Try og:image meta tag
        og_image = soup.find('meta', {'property': 'og:image'})
        if og_image and og_image.get('content'):
            return og_image['content']
                
        return None
    
    def _extract_work_id(self, soup) -> str:
        """Extract work ID for editions/similar books"""
        next_data = soup.find('script', id='__NEXT_DATA__')
        if next_data:        
            try:
                data = json.loads(next_data.string)
                for key, value in data['props']['pageProps']['apolloState'].items():
                    if isinstance(value, dict) and 'editions' in value:
                        if value['editions'].get('webUrl'):
                            return value['editions']['webUrl'].split('/')[-1].split('-')[0]
            except (json.JSONDecodeError, KeyError, AttributeError):
                pass
        return None


# core/scrapers/editions_scraper.py
from bs4 import BeautifulSoup
from pathlib import Path
import re
from urllib.parse import urlencode
from ..utils.http import GoodreadsDownloader

class EditionsScraper:
    """Scrapes editions from a Goodreads work page"""
    
    def __init__(self, scrape: bool = False):
        self.downloader = GoodreadsDownloader(scrape)
        self.has_english_editions = False
        self.has_valid_format = False
        self.has_page_count = False
        self.has_valid_publication = False
    
    def scrape_editions(self, work_id: str) -> list[dict]:
        """
        Get editions from first page of a work
        Returns list of editions with format:
        [
            {
                'goodreads_id': str,
                'title': str,
                'format': str,  # Paperback, Hardcover, etc.
                'pages': int,
                'published_date': str,
                'language': str,
                'rating': float,
                'rating_count': int
            }
        ]
        """
        # Reset all flags
        self.has_english_editions = False
        self.has_valid_format = False
        self.has_page_count = False
        self.has_valid_publication = False
        
        # Get first page content
        url = self._get_page_url(work_id, 1)
        if not self.downloader.download_url(url):
            return []
        
        # Read the downloaded page
        html = self._read_html(work_id, 1)
        if not html:
            return []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            editions = self._extract_editions(soup)
            return editions
            
        except Exception as e:
            print(f"Error processing page: {e}")
            return []
    
    def _get_page_url(self, work_id: str, page: int) -> str:
        """Get URL for editions page"""
        base = f"https://www.goodreads.com/work/editions/{work_id}"
        params = {
            'page': page,
            'per_page': 100,
            'utf8': 'âœ“',
            'expanded': 'true'
        }
        return f"{base}?{urlencode(params)}"
    
    def _read_html(self, work_id: str, page: int) -> str:
        """Read downloaded HTML file"""
        query = f"page={page}&per_page=100&utf8=%E2%9C%93&expanded=true"
        path = Path('data/cache/work/editions') / f"{work_id}{query}.html"
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"Error reading HTML file: {e}")
            return None
    
    def _extract_editions(self, soup) -> list:
        """Extract editions from page"""
        editions = []
        edition_elements = soup.find_all('div', class_='elementList clearFix')
        
        valid_formats = ['Kindle Edition', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'ebook']
        
        for element in edition_elements:
            edition = {
                'goodreads_id': None,
                'title': None,
                'format': None,
                'pages': None,
                'published_date': None,
                'language': None,
                'rating': None,
                'rating_count': None
            }
            
            # Get ID and title
            book_link = element.find('a', class_='bookTitle')
            if book_link:
                edition['title'] = book_link.text.strip()
                url_match = re.search(r'/show/(\d+)', book_link['href'])
                if url_match:
                    edition['goodreads_id'] = url_match.group(1)
            
            # Get other details
            details = element.find('div', class_='editionData')
            if details:
                # Get format and pages
                format_div = details.find('div', text=re.compile(r'(Paperback|Hardcover|Kindle Edition|ebook|Mass Market Paperback).*pages?', re.IGNORECASE))
                if format_div:
                    text = format_div.text.strip()
                    format_match = re.search(r'(Paperback|Hardcover|Kindle Edition|ebook|Mass Market Paperback)', text, re.IGNORECASE)
                    if format_match:
                        edition['format'] = format_match.group(1)
                        # Track if we've found a valid format
                        if edition['format'] in valid_formats:
                            self.has_valid_format = True
                    
                    pages_match = re.search(r'(\d+)\s*pages?', text)
                    if pages_match:
                        edition['pages'] = int(pages_match.group(1))
                        # Track if we've found any page counts
                        if edition['pages'] > 0:
                            self.has_page_count = True
                
                # Get published date
                pub_div = details.find('div', text=re.compile(r'Published|Expected publication'))
                if pub_div:
                    text = pub_div.text.strip()
                    # Extract date portion, excluding "by Publisher" text
                    date_match = re.search(r'(?:Published|Expected publication)\s+(.*?)(?:\s+by\s+.*)?$', text)
                    if date_match:
                        date_text = date_match.group(1).strip()
                        # Only set if it looks like a real date (not just "by Publisher")
                        if not date_text.startswith('by '):
                            edition['published_date'] = date_text
                            # Track if we've found any valid publication dates
                            self.has_valid_publication = True
                
                # Get language
                language_div = details.find('div', text=lambda x: x and 'Edition language:' in x)
                if language_div and language_div.find_next('div', class_='dataValue'):
                    edition['language'] = language_div.find_next('div', class_='dataValue').text.strip()
                    # Track if we've found any English editions
                    if edition['language'] == 'English':
                        self.has_english_editions = True
                
                # Get rating and rating count
                rating_div = details.find('div', text=lambda x: x and 'Average rating:' in x)
                if rating_div:
                    rating_value = rating_div.find_next('div', class_='dataValue')
                    if rating_value:
                        text = rating_value.text.strip()
                        rating_match = re.search(r'(\d+\.\d+)\s*\(([0-9,]+)\s*ratings?\)', text)
                        if rating_match:
                            edition['rating'] = float(rating_match.group(1))
                            edition['rating_count'] = int(rating_match.group(2).replace(',', ''))
            
            # Only add if it meets all criteria:
            # - Has goodreads_id and title
            # - Has pages
            # - Has valid published date (not just "by Publisher")
            # - Is in English
            # - Has valid format
            if (edition['goodreads_id'] and edition['title'] and 
                edition['pages'] and
                edition['published_date'] and
                not edition['published_date'].startswith('by ') and
                edition['language'] == 'English' and
                edition['format'] in valid_formats):
                editions.append(edition)
        
        return editions


# core\scrapers\list_scraper.py

from bs4 import BeautifulSoup
from pathlib import Path
import re
from .base_scraper import BaseScraper

class ListScraper(BaseScraper):
    """Scrapes books from a Goodreads list page"""
    
    def __init__(self, scrape: bool = False):
        super().__init__(scrape, cache_dir='data/cache/list/show')
    
    def get_url(self, identifier: str) -> str:
        """Get URL for list page (required by BaseScraper)"""
        return self.get_page_url(identifier, 1)
    
    def get_cache_path(self, identifier: str, subdir: str = '', suffix: str = '.html') -> Path:
        """Override cache path to handle first page differently"""
        path = self.cache_dir
        if subdir:
            path = path / subdir
            
        # First page doesn't have page parameter in URL
        if identifier.endswith('_page_1'):
            base_id = identifier.replace('_page_1', '')
            return path / f"{base_id}.html"
            
        # For other pages, include the page parameter
        if '_page_' in identifier:
            base_id, page = identifier.split('_page_')
            return self.cache_dir / f"{base_id}page={page}.html"
            
        return path / f"{identifier}{suffix}"
    
    def extract_data(self, soup: BeautifulSoup, identifier: str) -> dict:
        """Extract data from a single page (required by BaseScraper)"""
        return {
            'items': self.extract_page_data(soup, identifier)
        }
    
    def scrape_list(self, list_id: str, max_pages: int = 1) -> list[dict]:
        """
        Get books from a list page
        Args:
            list_id: The Goodreads list ID
            max_pages: Maximum number of pages to scrape (default: 1)
        
        Returns list of books with format:
        [
            {
                'goodreads_id': str,
                'title': str,
                'author': {
                    'goodreads_id': str,
                    'name': str
                },
                'rating': float,
                'rating_count': int,
                'score': int,
                'votes': int
            }
        ]
        """
        self.logger.info(f"Scraping list: {list_id}")
        
        # Use base class pagination
        result = self.scrape_paginated(list_id, max_pages)
        if not result:
            self.logger.warning(f"No results found for list: {list_id}")
            return []
            
        if not result.get('items'):
            self.logger.warning(f"No books found in list: {list_id}")
            return []
            
        self.logger.info(f"Found {len(result['items'])} books in list")
        return result['items']
    
    def get_page_url(self, list_id: str, page: int) -> str:
        """Get URL for list page"""
        base_url = f"https://www.goodreads.com/list/show/{list_id}"
        if page > 1:
            return f"{base_url}?page={page}"
        return base_url
    
    def get_pagination_params(self, page: int) -> dict:
        """Get pagination parameters for URL"""
        # Only include page parameter for pages after first
        if page > 1:
            return {'page': page}
        return {}
    
    def download_url(self, url: str, identifier: str, retries: int = 3) -> bool:
        """Override download to handle rate limiting and retries"""
        self.logger.info(f"Downloading: {url}")
        success = self.downloader.download_url(url)
        if not success:
            self.logger.error(f"Failed to download: {url}")
            return False
        
        # Get the expected cache path
        final_path = self.get_cache_path(identifier)
        
        # Small delay to ensure file is closed
        import time
        time.sleep(0.1)
        
        try:
            # Read from the downloader's cache location
            # For paginated requests, handle both formats
            if '_page_' in identifier:
                base_id, page = identifier.split('_page_')
                temp_path = Path('data/cache/list/show') / f"{base_id}page={page}.html"
            else:
                temp_path = Path('data/cache/list/show') / f"{identifier}.html"
            
            if temp_path.exists():
                # Create parent directories if they don't exist
                final_path.parent.mkdir(parents=True, exist_ok=True)
                
                # Read and write in one go to avoid file locking issues
                with open(temp_path, 'r', encoding='utf-8') as src:
                    content = src.read()
                    
                # Write to the correct final path
                with open(final_path, 'w', encoding='utf-8') as dst:
                    dst.write(content)
                    
                # Also write to base path for first page to maintain compatibility
                if not '_page_' in identifier:
                    base_path = Path('data/cache/list/show') / f"{identifier}.html"
                    if base_path != final_path:
                        with open(base_path, 'w', encoding='utf-8') as base:
                            base.write(content)
                
                self.logger.info(f"Saved cache file to: {final_path}")
                return True
                
            self.logger.error(f"Downloaded file not found: {temp_path}")
            return False
            
        except Exception as e:
            self.logger.error(f"Failed to save cache file: {e}")
            return False
    
    def extract_page_data(self, soup: BeautifulSoup, identifier: str) -> list:
        """Extract books from list page"""
        books = []
        book_rows = soup.find_all('tr', itemtype="http://schema.org/Book")
        
        if not book_rows:
            self.logger.warning(f"No book rows found on page: {identifier}")
            return []
            
        self.logger.info(f"Found {len(book_rows)} book rows on page")
        
        for row in book_rows:
            book = {
                'goodreads_id': None,
                'title': None,
                'author': {
                    'goodreads_id': None,
                    'name': None
                },
                'rating': None,
                'rating_count': None,
                'score': None,
                'votes': None
            }
            
            # Get book ID and title
            title_link = row.find('a', class_='bookTitle')
            if title_link:
                book['title'] = title_link.find('span', itemprop='name').text.strip()
                url_match = re.search(r'/show/(\d+)', title_link['href'])
                if url_match:
                    book['goodreads_id'] = url_match.group(1)
            
            # Get author info
            author_link = row.find('a', class_='authorName')
            if author_link:
                book['author']['name'] = author_link.find('span', itemprop='name').text.strip()
                author_url_match = re.search(r'/author/show/(\d+)', author_link['href'])
                if author_url_match:
                    book['author']['goodreads_id'] = author_url_match.group(1)
            
            # Get rating and rating count
            rating_text = row.find('span', class_='minirating')
            if rating_text:
                text = rating_text.text.strip()
                rating_match = re.search(r'(\d+\.\d+)\s*avg rating\s*â€”\s*([\d,]+)\s*ratings', text)
                if rating_match:
                    book['rating'] = float(rating_match.group(1))
                    book['rating_count'] = int(rating_match.group(2).replace(',', ''))
            
            # Get score and votes
            score_text = row.find('a', onclick=lambda x: x and 'score_explanation' in x)
            if score_text:
                score_match = re.search(r'score:\s*([\d,]+)', score_text.text)
                if score_match:
                    book['score'] = int(score_match.group(1).replace(',', ''))
            
            votes_text = row.find('a', id=lambda x: x and x.startswith('loading_link_'))
            if votes_text:
                votes_match = re.search(r'(\d+)\s*people voted', votes_text.text)
                if votes_match:
                    book['votes'] = int(votes_match.group(1))
            
            # Only add if we have the essential data
            if book['goodreads_id'] and book['title']:
                books.append(book)
        
        return books 


# core/scrapers/series_scraper.py
from bs4 import BeautifulSoup
from pathlib import Path
import re
from ..utils.http import GoodreadsDownloader

class SeriesScraper:
   """Scrapes series information and books"""
   
   def __init__(self, scrape: bool = False):
       self.downloader = GoodreadsDownloader(scrape)
   
   def scrape_series(self, series_id: str) -> dict:
       """
       Get series info and books
       Expected output:
       {
           'goodreads_id': str,
           'name': str, 
           'books': [
               {
                   'goodreads_id': str,
                   'title': str,
                   'order': float
               }
           ]
       }
       """
       print(f"Scraping series: {series_id}")
       
       # Get series page
       url = self._get_series_url(series_id)
       if not self.downloader.download_url(url):
           print(f"Failed to download series page for ID: {series_id}")
           return None
           
       # Read HTML
       html = self._read_html(series_id)
       if not html:
           return None
           
       try:
           soup = BeautifulSoup(html, 'html.parser')
           
           series_data = {
               'goodreads_id': series_id,
               'name': self._extract_name(soup),
               'books': self._extract_books(soup)
           }
           
           return series_data
           
       except Exception as e:
           print(f"Error parsing series data: {e}")
           return None
   
   def _get_series_url(self, series_id: str) -> str:
       """Get Goodreads URL for series"""
       return f"https://www.goodreads.com/series/show/{series_id}"
   
   def _read_html(self, series_id: str) -> str:
       """Read downloaded HTML file"""
       path = Path('data/cache/series/show') / f"{series_id}.html"
       try:
           with open(path, 'r', encoding='utf-8') as f:
               return f.read()
       except Exception as e:
           print(f"Error reading HTML file: {e}")
           return None
   
   def _extract_name(self, soup) -> str:
       """Extract series name"""
       title_element = soup.find('h1', class_='gr-h1--serif')
       if title_element:
           name = title_element.text.strip()
           # Remove " Series" from the end if present
           if name.endswith(' Series'):
               name = name[:-7]
           return name
       return None
   
   def _extract_books(self, soup) -> list:
       """Extract books in series with order"""
       books = []
       book_divs = soup.find_all('div', class_='listWithDividers__item')
       
       for book_div in book_divs:
           # Get order number
           order = None
           number_heading = book_div.find('h3', class_='gr-h3--noBottomMargin')
           if number_heading:
               number_text = number_heading.text.strip()
               try:
                   if number_text.startswith('Book '):
                       number_text = number_text[5:]
                   if '-' not in number_text:  # Skip ranges like "1-3"
                       order = float(number_text)
               except ValueError:
                   pass
           
           # Get title and ID
           title_link = book_div.find('a', class_='gr-h3--serif')
           if title_link:
               title = title_link.find('span', itemprop='name')
               if title:
                   # Extract ID from URL
                   url_match = re.search(r'/show/(\d+)', title_link['href'])
                   if url_match:
                       books.append({
                           'goodreads_id': url_match.group(1),
                           'title': title.text.strip(),
                           'order': order
                       })
       
       return books


# core/scrapers/similar_scraper.py
from bs4 import BeautifulSoup
from pathlib import Path
import re
from ..utils.http import GoodreadsDownloader

class SimilarScraper:
    """Scrapes similar books from Goodreads"""
    
    def __init__(self, scrape: bool = False):
        self.downloader = GoodreadsDownloader(scrape)
    
    def scrape_similar_books(self, work_id: str) -> list[dict]:
        """
        Get similar books for a given book
        Returns list of books with format:
        [
            {
                'goodreads_id': str,
                'title': str
            }
        ]
        """
        print(f"Scraping similar books for: {work_id}")
        
        # Get page content
        url = self._get_page_url(work_id)
        if not self.downloader.download_url(url):
            print(f"Failed to download similar books page for ID: {work_id}")
            return []
            
        # Read the downloaded page
        html = self._read_html(work_id)
        if not html:
            return []
            
        try:
            soup = BeautifulSoup(html, 'html.parser')
            return self._extract_similar_books(soup)
            
        except Exception as e:
            print(f"Error processing similar books: {e}")
            return []
    
    def _get_page_url(self, work_id: str) -> str:
        """Get URL for similar books page"""
        return f"https://www.goodreads.com/book/similar/{work_id}"
    
    def _read_html(self, work_id: str) -> str:
        """Read downloaded HTML file"""
        path = Path('data/cache/book/similar') / f"{work_id}.html"
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"Error reading HTML file: {e}")
            return None
    
    def _extract_similar_books(self, soup) -> list:
        """Extract similar books from page"""
        similar_books = []
        book_divs = soup.find_all('div', class_='u-paddingBottomXSmall')
        
        # Skip first div (it's the source book)
        for div in book_divs[1:]:
            book_link = div.find('a', class_='gr-h3')
            if not book_link:
                continue
                
            book = {
                'goodreads_id': None,
                'title': None
            }
            
            # Get ID from URL
            url_match = re.search(r'/show/(\d+)', book_link['href'])
            if url_match:
                book['goodreads_id'] = url_match.group(1)
            
            # Get title
            title_span = book_link.find('span', itemprop='name')
            if title_span:
                book['title'] = title_span.text.strip()
            
            if book['goodreads_id'] and book['title']:
                similar_books.append(book)
        
        return similar_books

