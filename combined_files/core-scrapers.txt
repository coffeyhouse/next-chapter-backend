# core/scrapers/author_books_scraper.py
from bs4 import BeautifulSoup
from pathlib import Path
import re
from urllib.parse import urlencode
from ..utils.http import GoodreadsDownloader

class AuthorBooksScraper:
   """Scrapes list of books by an author"""
   
   def __init__(self, scrape: bool = False):
       self.downloader = GoodreadsDownloader(scrape)
   
   def scrape_author_books(self, author_id: str) -> dict:
       """
       Get list of all books by author
       Expected output:
       {
           'author_id': str,
           'author_name': str,
           'books': [
               {
                   'goodreads_id': str,
                   'title': str,
                   'published_date': str
               }
           ]
       }
       """
       print(f"Scraping books for author: {author_id}")
       books = []
       author_name = None
       current_page = 1
       
       while True:
           # Get page content
           url = self._get_page_url(author_id, current_page)
           if not self.downloader.download_url(url):
               if books:  # Return what we have if not first page
                   break
               return None
           
           # Read the downloaded page
           html = self._read_html(author_id, current_page)
           if not html:
               if books:
                   break
               return None
               
           try:
               # Parse page content
               soup = BeautifulSoup(html, 'html.parser')
               
               # Get author name on first page
               if not author_name:
                   author_name = self._extract_author_name(soup)
               
               # Get books from this page
               page_books = self._extract_books(soup)
               books.extend(page_books)
               
               # Check pagination
               pagination = self._extract_pagination(soup)
               print(f"Processing page {pagination['current_page']} of {pagination['total_pages']}")
               
               if current_page >= pagination['total_pages']:
                   break
                   
               current_page += 1
               
           except Exception as e:
               print(f"Error processing page {current_page}: {e}")
               if books:
                   break
               return None
       
       # Filter to only include books with dates
       dated_books = [b for b in books if b.get('published_date')]
       
       return {
           'author_id': author_id,
           'author_name': author_name,
           'books': dated_books
       }
   
   def _get_page_url(self, author_id: str, page: int) -> str:
       """Get URL for author's books page"""
       base = f"https://www.goodreads.com/author/list/{author_id}"
       params = {
           'page': page,
           'per_page': 100,
           'utf8': 'âœ“',
           'sort': 'original_publication_year'
       }
       return f"{base}?{urlencode(params)}"
   
   def _read_html(self, author_id: str, page: int) -> str:
       """Read downloaded HTML file"""
       query = f"page={page}&per_page=100&utf8=%E2%9C%93&sort=original_publication_year"
       path = Path('data/cache/author/list') / f"{author_id}{query}.html"
       try:
           with open(path, 'r', encoding='utf-8') as f:
               return f.read()
       except Exception as e:
           print(f"Error reading HTML file: {e}")
           return None
   
   def _extract_author_name(self, soup) -> str:
       """Extract author name"""
       name_link = soup.find('a', class_='authorName')
       if name_link:
           return name_link.text.strip()
       return None
   
   def _extract_books(self, soup) -> list:
       """Extract books from page"""
       books = []
       book_rows = soup.find_all('tr', itemtype='http://schema.org/Book')
       
       for row in book_rows:
           book_link = row.find('a', class_='bookTitle')
           if not book_link:
               continue
               
           # Get title and ID
           book = {
               'title': book_link.find('span', itemprop='name').text.strip(),
               'goodreads_id': None,
               'published_date': None
           }
           
           # Get book ID from URL
           url_match = re.search(r'/show/(\d+)', book_link['href'])
           if url_match:
               book['goodreads_id'] = url_match.group(1)
               
           # Get publication date
           grey_text = row.find('span', class_='greyText smallText uitext')
           if grey_text:
               text = grey_text.get_text(strip=True)
               for prefix in ['expected publication', 'published']:
                   if prefix in text.lower():
                       year_match = re.search(rf'{prefix}\s*(\d{{4}})', text, re.I)
                       if year_match:
                           book['published_date'] = year_match.group(1)
                           break
           
           if book['goodreads_id'] and book['title']:
               books.append(book)
               
       return books
   
   def _extract_pagination(self, soup) -> dict:
       """Extract pagination information"""
       pagination = {'current_page': 1, 'total_pages': 1}
       
       # Find pagination div
       div = soup.find('div', style='float: right')
       if div:
           # Get current page
           current = div.find('em', class_='current')
           if current:
               try:
                   pagination['current_page'] = int(current.text.strip())
               except ValueError:
                   pass
           
           # Get max page number
           max_page = 1
           for link in div.find_all('a'):
               try:
                   page_num = int(link.text.strip())
                   max_page = max(max_page, page_num)
               except ValueError:
                   continue
                   
           pagination['total_pages'] = max_page
           
       return pagination


# core/scrapers/author_scraper.py
from bs4 import BeautifulSoup
from pathlib import Path
from ..utils.http import GoodreadsDownloader
from ..utils.image import download_author_photo

class AuthorScraper:
    """Scrapes author details from Goodreads"""
    
    def __init__(self, scrape: bool = False):
        self.downloader = GoodreadsDownloader(scrape)
    
    def scrape_author(self, author_id: str) -> dict:
        """
        Get author data from author page
        Expected output matches authors table schema:
        {
            'goodreads_id': str,
            'name': str,
            'bio': str,
            'image_url': str
        }
        """
        print(f"Scraping author: {author_id}")
        
        # Get author page
        url = self._get_author_url(author_id)
        if not self.downloader.download_url(url):
            print(f"Failed to download author page for ID: {author_id}")
            return None
            
        # Read HTML
        html = self._read_html(author_id)
        if not html:
            return None
            
        try:
            soup = BeautifulSoup(html, 'html.parser')
            author_data = {
                'goodreads_id': author_id,
                'name': self._extract_name(soup),
                'bio': self._extract_bio(soup),
                'image_url': None
            }
            
            # Handle author photo
            photo_url = self._extract_photo_url(soup)
            if photo_url:
                local_path = download_author_photo(author_id, photo_url)
                if local_path:
                    author_data['image_url'] = local_path
            
            return author_data
            
        except Exception as e:
            print(f"Error parsing author data: {e}")
            return None
    
    def _get_author_url(self, author_id: str) -> str:
        """Get Goodreads URL for author"""
        return f"https://www.goodreads.com/author/show/{author_id}"
    
    def _read_html(self, author_id: str) -> str:
        """Read downloaded HTML file"""
        path = Path('data/cache/author/show') / f"{author_id}.html"
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"Error reading HTML file: {e}")
            return None
    
    def _extract_name(self, soup) -> str:
        """Extract author name"""
        name_element = soup.find('h1', class_='authorName')
        if name_element:
            name_span = name_element.find('span', itemprop='name')
            if name_span:
                return ' '.join(name_span.text.split())
        return None
    
    def _extract_bio(self, soup) -> str:
        """Extract author biography"""
        bio_div = soup.find('div', class_='aboutAuthorInfo')
        if bio_div:
            bio_span = bio_div.find('span', id=lambda x: x and x.startswith('freeTextContainer'))
            if bio_span:
                return bio_span.text.strip()
        return None
    
    def _extract_photo_url(self, soup) -> str:
        """Extract author photo URL with fallbacks"""
        # Try authorPhotoImg class first
        img = soup.find('img', class_='authorPhotoImg')
        if img and 'src' in img.attrs:
            return img['src']
        
        # Try alt text containing author
        img = soup.find('img', alt=lambda x: x and 'author' in x.lower())
        if img and 'src' in img.attrs:
            return img['src']
        
        # Try leftContainer
        container = soup.find('div', class_='leftContainer authorLeftContainer')
        if container:
            img = container.find('img')
            if img and 'src' in img.attrs:
                return img['src']
        
        return None


# core/scrapers/book_scraper.py
from bs4 import BeautifulSoup
from pathlib import Path
import re
import json
from datetime import datetime
import click
from ..utils.http import GoodreadsDownloader
from ..utils.image import download_book_cover

class BookScraper:
    """Scrapes a book page from Goodreads"""
    
    def __init__(self, scrape: bool = False):
        self.downloader = GoodreadsDownloader(scrape)
        
    def scrape_book(self, book_id: str) -> dict:
        """
        Get book data from Goodreads book page
        Expected output:
        {
            'goodreads_id': str,
            'title': str,
            'work_id': str,
            'published_date': str,
            'published_state': str,
            'language': str,
            'pages': int,
            'isbn': str,
            'goodreads_rating': float,
            'goodreads_votes': int,
            'description': str,
            'image_url': str,
            'source': str,
            'hidden': bool,
            # Relationships (not in books table)
            'authors': [
                {
                    'goodreads_id': str,
                    'name': str,
                    'role': str
                }
            ],
            'series': [
                {
                    'goodreads_id': str,
                    'name': str,
                    'order': float
                }
            ],
            'genres': [
                {
                    'name': str
                }
            ]
        }
        """
        if click.get_current_context().find_root().params.get('verbose', False):
            click.echo(click.style(f"Scraping book: {book_id}", fg='cyan'))
        
        # Get book page content
        url = self._get_book_url(book_id)
        if not self.downloader.download_url(url):
            click.echo(click.style(f"Failed to download book page for ID: {book_id}", fg='red'), err=True)
            return None
            
        # Parse HTML
        html = self._read_html(book_id)
        if not html:
            return None
            
        try:
            soup = BeautifulSoup(html, 'html.parser')
            book_data = {
            'goodreads_id': book_id,
            'title': self._extract_title(soup),
            'work_id': self._extract_work_id(soup),
            'description': self._extract_description(soup),
            'language': None,
            'pages': None,
            'isbn': None,
            'goodreads_rating': None,
            'goodreads_votes': None,
            'published_date': None,
            'published_state': None,
            'image_url': None,
            'source': 'scrape',
            'hidden': False
        }
            
            # Extract book details
            details = self._extract_book_details(soup)
            book_data.update({
                'language': details.get('language'),
                'pages': details.get('pages'),
                'isbn': details.get('isbn'),
                'goodreads_rating': details.get('rating'),
                'goodreads_votes': details.get('rating_count')
            })
            
            # Get publication date
            pub_info = self._extract_publication_info(soup)
            book_data['published_date'] = pub_info.get('date')
            book_data['published_state'] = pub_info.get('state')
            
            # Get relationships
            book_data['authors'] = self._extract_authors(soup)
            book_data['series'] = self._extract_series(soup)
            book_data['genres'] = self._extract_genres(soup)
            
            # Get cover image
            cover_url = self._extract_cover_url(soup)
            if cover_url:
                local_path = download_book_cover(book_data['work_id'], cover_url)
                if local_path:
                    book_data['image_url'] = local_path
            
            return book_data
            
        except Exception as e:
            click.echo(click.style(f"Error parsing book data: {e}", fg='red'), err=True)
            return None
    
    def _get_book_url(self, book_id: str) -> str:
        """Get Goodreads URL for book"""
        return f"https://www.goodreads.com/book/show/{book_id}"
    
    def _read_html(self, book_id: str) -> str:
        """Read downloaded HTML file"""
        path = Path('data/cache/book/show') / f"{book_id}.html"
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            click.echo(click.style(f"Error reading HTML file: {e}", fg='red'), err=True)
            return None
    
    def _extract_title(self, soup) -> str:
        """Extract book title"""
        title_element = soup.find('h1', attrs={'data-testid': 'bookTitle'})
        if title_element:
            return title_element['aria-label'].replace('Book title:', '').strip()
        return None
    
    def _extract_description(self, soup) -> str:
        """Extract book description"""
        next_data = soup.find('script', id='__NEXT_DATA__')
        if next_data:
            try:
                data = json.loads(next_data.string)
                apollo_state = data['props']['pageProps']['apolloState']
                
                # Find the book ID from the query
                book_id = None
                for key, value in apollo_state.items():
                    if key == 'ROOT_QUERY':
                        for query_key in value:
                            if 'getBookByLegacyId' in query_key:
                                book_id = value[query_key]['__ref']
                                break
                
                # Get description using book ID
                if book_id and book_id in apollo_state:
                    return apollo_state[book_id].get('description')
                    
            except (json.JSONDecodeError, KeyError):
                pass
        return None
    def _extract_book_details(self, soup) -> dict:
        """Extract book details from schema.org data"""
        schema = soup.find('script', {'type': 'application/ld+json'})
        if schema:
            try:
                data = json.loads(schema.string)
                return {
                    'format': data.get('bookFormat'),
                    'language': data.get('inLanguage'),
                    'pages': data.get('numberOfPages'),
                    'isbn': data.get('isbn'),
                    'rating': data.get('aggregateRating', {}).get('ratingValue'),
                    'rating_count': data.get('aggregateRating', {}).get('ratingCount')
                }
            except json.JSONDecodeError:
                pass
        return {}
    
    def _extract_publication_info(self, soup) -> dict:
        pub_element = soup.find('p', attrs={'data-testid': 'publicationInfo'})
        if pub_element:
            text = pub_element.text.strip()
            result = {'date': None, 'state': None}

            # Determine the raw date string based on the prefix
            if text.startswith('Expected publication'):
                result['state'] = 'upcoming'
                raw_date = text.replace('Expected publication', '').strip()
            elif text.startswith('First published'):
                result['state'] = 'published'
                raw_date = text.replace('First published', '').strip()
            elif text.startswith('Published'):
                result['state'] = 'published'
                raw_date = text.replace('Published', '').strip()
            else:
                raw_date = text

            # Convert the raw date to ISO 8601 format if possible.
            try:
                # Adjust the format string if your raw date differs.
                dt = datetime.strptime(raw_date, "%B %d, %Y")
                # This produces something like "2021-08-05T00:00:00.000000"
                result['date'] = dt.isoformat(timespec='microseconds')
            except Exception as e:
                # If parsing fails, fall back to the raw string
                result['date'] = raw_date

            return result
        return {}
    
    def _extract_authors(self, soup) -> list:
        """Extract unique author information"""
        seen_ids = set()  # Track seen author IDs
        authors = []
        author_links = soup.find_all('a', class_='ContributorLink')
        
        for link in author_links:
            name_span = link.find('span', class_='ContributorLink__name')
            if not name_span:
                continue
                
            # Get author ID first
            goodreads_id = None
            if 'href' in link.attrs:
                id_match = re.search(r'/author/show/(\d+)', link['href'])
                if id_match:
                    goodreads_id = id_match.group(1)
            
            # Skip if we don't have a goodreads_id or if we've seen this ID before
            if not goodreads_id or goodreads_id in seen_ids:
                continue
                
            author = {
                'name': name_span.text.strip(),
                'goodreads_id': goodreads_id,
                'role': 'Author'
            }
            
            # Get role if specified
            role_span = link.find('span', class_='ContributorLink__role')
            if role_span:
                role = role_span.text.strip('()').strip()
                # Clean up any remaining parentheses and whitespace
                role = role.replace('(', '').replace(')', '').strip()
                author['role'] = role
            
            authors.append(author)
            seen_ids.add(goodreads_id)
                
        return authors
    
    def _extract_series(self, soup) -> list:
        """Extract all series information (main and additional)"""
        series = []
        
        # First get the main series
        series_element = soup.find('h3', class_='Text__title3', 
                                attrs={'aria-label': lambda x: x and 'Book' in x and 'series' in x})
        
        main_series_id = None
        if series_element:
            series_link = series_element.find('a')
            if series_link:
                # Get series ID
                id_match = re.search(r'/series/(\d+)', series_link['href'])
                if id_match:
                    main_series_id = id_match.group(1)
                    text = series_link.text.strip()
                    
                    # Handle series order if present
                    name = text
                    order = None
                    if '#' in text:
                        name, number = text.split('#')
                        try:
                            order = float(number.strip())
                        except ValueError:
                            pass
                    
                    series.append({
                        'goodreads_id': main_series_id,
                        'name': name.strip(),
                        'order': order
                    })
        
        # Then get additional series
        next_data = soup.find('script', id='__NEXT_DATA__')
        if next_data:
            try:
                data = json.loads(next_data.string)
                book_data = data.get('props', {}).get('pageProps', {}).get('apolloState', {})
                
                # Get all series
                for key, value in book_data.items():
                    if isinstance(value, dict) and value.get('__typename') == 'Series':
                        url = value.get('webUrl', '')
                        series_match = re.search(r'/series/(\d+)', url)
                        series_id = series_match.group(1) if series_match else None
                        
                        # Skip if this is the main series
                        if main_series_id and str(main_series_id) == str(series_id):
                            continue
                        
                        # Add to series list with consistent format
                        series.append({
                            'goodreads_id': series_id,
                            'name': value.get('title', ''),
                            'order': None  # Additional series typically don't have order
                        })
                        
            except json.JSONDecodeError:
                pass
        
        return series
    
    def _extract_genres(self, soup) -> list:
        """Extract genre information"""
        genres = []
        next_data = soup.find('script', id='__NEXT_DATA__')
        if next_data:
            try:
                data = json.loads(next_data.string)
                book_data = data['props']['pageProps']['apolloState']
                
                for value in book_data.values():
                    if isinstance(value, dict) and 'bookGenres' in value:
                        for genre_data in value['bookGenres']:
                            if isinstance(genre_data, dict) and 'genre' in genre_data:
                                genre = genre_data['genre']
                                if isinstance(genre, dict):
                                    genres.append({
                                        'name': genre.get('name', '')
                                    })
                        break
            except (json.JSONDecodeError, KeyError):
                pass
        return genres
    
    def _extract_cover_url(self, soup) -> str:
        """Extract book cover URL"""
        # Try to get from __NEXT_DATA__ first (new Goodreads structure)
        next_data = soup.find('script', id='__NEXT_DATA__')
        if next_data:
            try:
                data = json.loads(next_data.string)
                apollo_state = data['props']['pageProps']['apolloState']
                
                # Find the book ID from the query
                book_id = None
                for key, value in apollo_state.items():
                    if key == 'ROOT_QUERY':
                        for query_key in value:
                            if 'getBookByLegacyId' in query_key:
                                book_id = value[query_key]['__ref']
                                break
                
                # Get image URL using book ID
                if book_id and book_id in apollo_state:
                    image_url = apollo_state[book_id].get('imageUrl')
                    if image_url:
                        return image_url
            except (json.JSONDecodeError, KeyError):
                pass
                
        # Try ResponsiveImage class
        img = soup.find('img', {'class': 'ResponsiveImage'})
        if img and 'src' in img.attrs:
            return img['src']
            
        # Try schema.org data
        schema = soup.find('script', {'type': 'application/ld+json'})
        if schema:
            try:
                data = json.loads(schema.string)
                image_url = data.get('image')
                if image_url:
                    return image_url
            except json.JSONDecodeError:
                pass
                
        # Try og:image meta tag
        og_image = soup.find('meta', {'property': 'og:image'})
        if og_image and og_image.get('content'):
            return og_image['content']
                
        return None
    
    def _extract_work_id(self, soup) -> str:
        """Extract work ID for editions/similar books"""
        next_data = soup.find('script', id='__NEXT_DATA__')
        if next_data:        
            try:
                data = json.loads(next_data.string)
                for key, value in data['props']['pageProps']['apolloState'].items():
                    if isinstance(value, dict) and 'editions' in value:
                        if value['editions'].get('webUrl'):
                            return value['editions']['webUrl'].split('/')[-1].split('-')[0]
            except (json.JSONDecodeError, KeyError, AttributeError):
                pass
        return None


# core/scrapers/editions_scraper.py
from bs4 import BeautifulSoup
from pathlib import Path
import re
from urllib.parse import urlencode
from ..utils.http import GoodreadsDownloader

class EditionsScraper:
    """Scrapes editions from a Goodreads work page"""
    
    def __init__(self, scrape: bool = False):
        self.downloader = GoodreadsDownloader(scrape)
    
    def scrape_editions(self, work_id: str) -> list[dict]:
        """
        Get editions from first page of a work
        Returns list of editions with format:
        [
            {
                'goodreads_id': str,
                'title': str,
                'format': str,  # Paperback, Hardcover, etc.
                'pages': int,
                'published_date': str,
                'language': str,
                'rating': float,
                'rating_count': int
            }
        ]
        """
        
        # Get first page content
        url = self._get_page_url(work_id, 1)
        if not self.downloader.download_url(url):
            return []
        
        # Read the downloaded page
        html = self._read_html(work_id, 1)
        if not html:
            return []
        
        try:
            soup = BeautifulSoup(html, 'html.parser')
            editions = self._extract_editions(soup)
            return editions
            
        except Exception as e:
            print(f"Error processing page: {e}")
            return []
    
    def _get_page_url(self, work_id: str, page: int) -> str:
        """Get URL for editions page"""
        base = f"https://www.goodreads.com/work/editions/{work_id}"
        params = {
            'page': page,
            'per_page': 100,
            'utf8': 'âœ“',
            'expanded': 'true'
        }
        return f"{base}?{urlencode(params)}"
    
    def _read_html(self, work_id: str, page: int) -> str:
        """Read downloaded HTML file"""
        query = f"page={page}&per_page=100&utf8=%E2%9C%93&expanded=true"
        path = Path('data/cache/work/editions') / f"{work_id}{query}.html"
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"Error reading HTML file: {e}")
            return None
    
    def _extract_editions(self, soup) -> list:
        """Extract editions from page"""
        editions = []
        edition_elements = soup.find_all('div', class_='elementList clearFix')
        
        valid_formats = ['Kindle Edition', 'Paperback', 'Hardcover', 'Mass Market Paperback', 'ebook']
        
        for element in edition_elements:
            edition = {
                'goodreads_id': None,
                'title': None,
                'format': None,
                'pages': None,
                'published_date': None,
                'language': None,
                'rating': None,
                'rating_count': None
            }
            
            # Get ID and title
            book_link = element.find('a', class_='bookTitle')
            if book_link:
                edition['title'] = book_link.text.strip()
                url_match = re.search(r'/show/(\d+)', book_link['href'])
                if url_match:
                    edition['goodreads_id'] = url_match.group(1)
            
            # Get other details
            details = element.find('div', class_='editionData')
            if details:
                # Get format and pages
                format_div = details.find('div', text=re.compile(r'(Paperback|Hardcover|Kindle Edition|ebook|Mass Market Paperback).*pages?', re.IGNORECASE))
                if format_div:
                    text = format_div.text.strip()
                    format_match = re.search(r'(Paperback|Hardcover|Kindle Edition|ebook|Mass Market Paperback)', text, re.IGNORECASE)
                    if format_match:
                        edition['format'] = format_match.group(1)
                    
                    pages_match = re.search(r'(\d+)\s*pages?', text)
                    if pages_match:
                        edition['pages'] = int(pages_match.group(1))
                
                # Get published date
                pub_div = details.find('div', text=re.compile(r'Published|Expected publication'))
                if pub_div:
                    text = pub_div.text.strip()
                    # Extract date portion, excluding "by Publisher" text
                    date_match = re.search(r'(?:Published|Expected publication)\s+(.*?)(?:\s+by\s+.*)?$', text)
                    if date_match:
                        date_text = date_match.group(1).strip()
                        # Only set if it looks like a real date (not just "by Publisher")
                        if not date_text.startswith('by '):
                            edition['published_date'] = date_text
                
                # Get language
                language_div = details.find('div', text=lambda x: x and 'Edition language:' in x)
                if language_div and language_div.find_next('div', class_='dataValue'):
                    edition['language'] = language_div.find_next('div', class_='dataValue').text.strip()
                
                # Get rating and rating count
                rating_div = details.find('div', text=lambda x: x and 'Average rating:' in x)
                if rating_div:
                    rating_value = rating_div.find_next('div', class_='dataValue')
                    if rating_value:
                        text = rating_value.text.strip()
                        rating_match = re.search(r'(\d+\.\d+)\s*\(([0-9,]+)\s*ratings?\)', text)
                        if rating_match:
                            edition['rating'] = float(rating_match.group(1))
                            edition['rating_count'] = int(rating_match.group(2).replace(',', ''))
            
            # Only add if it meets all criteria:
            # - Has goodreads_id and title
            # - Has pages
            # - Has valid published date (not just "by Publisher")
            # - Is in English
            # - Has valid format
            if (edition['goodreads_id'] and edition['title'] and 
                edition['pages'] and
                edition['published_date'] and
                not edition['published_date'].startswith('by ') and
                edition['language'] == 'English' and
                edition['format'] in valid_formats):
                editions.append(edition)
        
        return editions


# core/scrapers/series_scraper.py
from bs4 import BeautifulSoup
from pathlib import Path
import re
from ..utils.http import GoodreadsDownloader

class SeriesScraper:
   """Scrapes series information and books"""
   
   def __init__(self, scrape: bool = False):
       self.downloader = GoodreadsDownloader(scrape)
   
   def scrape_series(self, series_id: str) -> dict:
       """
       Get series info and books
       Expected output:
       {
           'goodreads_id': str,
           'name': str, 
           'books': [
               {
                   'goodreads_id': str,
                   'title': str,
                   'order': float
               }
           ]
       }
       """
       print(f"Scraping series: {series_id}")
       
       # Get series page
       url = self._get_series_url(series_id)
       if not self.downloader.download_url(url):
           print(f"Failed to download series page for ID: {series_id}")
           return None
           
       # Read HTML
       html = self._read_html(series_id)
       if not html:
           return None
           
       try:
           soup = BeautifulSoup(html, 'html.parser')
           
           series_data = {
               'goodreads_id': series_id,
               'name': self._extract_name(soup),
               'books': self._extract_books(soup)
           }
           
           return series_data
           
       except Exception as e:
           print(f"Error parsing series data: {e}")
           return None
   
   def _get_series_url(self, series_id: str) -> str:
       """Get Goodreads URL for series"""
       return f"https://www.goodreads.com/series/show/{series_id}"
   
   def _read_html(self, series_id: str) -> str:
       """Read downloaded HTML file"""
       path = Path('data/cache/series/show') / f"{series_id}.html"
       try:
           with open(path, 'r', encoding='utf-8') as f:
               return f.read()
       except Exception as e:
           print(f"Error reading HTML file: {e}")
           return None
   
   def _extract_name(self, soup) -> str:
       """Extract series name"""
       title_element = soup.find('h1', class_='gr-h1--serif')
       if title_element:
           name = title_element.text.strip()
           # Remove " Series" from the end if present
           if name.endswith(' Series'):
               name = name[:-7]
           return name
       return None
   
   def _extract_books(self, soup) -> list:
       """Extract books in series with order"""
       books = []
       book_divs = soup.find_all('div', class_='listWithDividers__item')
       
       for book_div in book_divs:
           # Get order number
           order = None
           number_heading = book_div.find('h3', class_='gr-h3--noBottomMargin')
           if number_heading:
               number_text = number_heading.text.strip()
               try:
                   if number_text.startswith('Book '):
                       number_text = number_text[5:]
                   if '-' not in number_text:  # Skip ranges like "1-3"
                       order = float(number_text)
               except ValueError:
                   pass
           
           # Get title and ID
           title_link = book_div.find('a', class_='gr-h3--serif')
           if title_link:
               title = title_link.find('span', itemprop='name')
               if title:
                   # Extract ID from URL
                   url_match = re.search(r'/show/(\d+)', title_link['href'])
                   if url_match:
                       books.append({
                           'goodreads_id': url_match.group(1),
                           'title': title.text.strip(),
                           'order': order
                       })
       
       return books


# core/scrapers/similar_scraper.py
from bs4 import BeautifulSoup
from pathlib import Path
import re
from ..utils.http import GoodreadsDownloader

class SimilarScraper:
    """Scrapes similar books from Goodreads"""
    
    def __init__(self, scrape: bool = False):
        self.downloader = GoodreadsDownloader(scrape)
    
    def scrape_similar_books(self, work_id: str) -> list[dict]:
        """
        Get similar books for a given book
        Returns list of books with format:
        [
            {
                'goodreads_id': str,
                'title': str
            }
        ]
        """
        print(f"Scraping similar books for: {work_id}")
        
        # Get page content
        url = self._get_page_url(work_id)
        if not self.downloader.download_url(url):
            print(f"Failed to download similar books page for ID: {work_id}")
            return []
            
        # Read the downloaded page
        html = self._read_html(work_id)
        if not html:
            return []
            
        try:
            soup = BeautifulSoup(html, 'html.parser')
            return self._extract_similar_books(soup)
            
        except Exception as e:
            print(f"Error processing similar books: {e}")
            return []
    
    def _get_page_url(self, work_id: str) -> str:
        """Get URL for similar books page"""
        return f"https://www.goodreads.com/book/similar/{work_id}"
    
    def _read_html(self, work_id: str) -> str:
        """Read downloaded HTML file"""
        path = Path('data/cache/book/similar') / f"{work_id}.html"
        try:
            with open(path, 'r', encoding='utf-8') as f:
                return f.read()
        except Exception as e:
            print(f"Error reading HTML file: {e}")
            return None
    
    def _extract_similar_books(self, soup) -> list:
        """Extract similar books from page"""
        similar_books = []
        book_divs = soup.find_all('div', class_='u-paddingBottomXSmall')
        
        # Skip first div (it's the source book)
        for div in book_divs[1:]:
            book_link = div.find('a', class_='gr-h3')
            if not book_link:
                continue
                
            book = {
                'goodreads_id': None,
                'title': None
            }
            
            # Get ID from URL
            url_match = re.search(r'/show/(\d+)', book_link['href'])
            if url_match:
                book['goodreads_id'] = url_match.group(1)
            
            # Get title
            title_span = book_link.find('span', itemprop='name')
            if title_span:
                book['title'] = title_span.text.strip()
            
            if book['goodreads_id'] and book['title']:
                similar_books.append(book)
        
        return similar_books

